{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9528f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f8b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('ML_Data_48.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3723713",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Z_jerk_mean', 'X_jerk_mean', 'total_jerk_mean', 'Y_jerk_mean','X_jerk_mean','X_jerk_min', 'X_min', 'timestamp_range', 'Y_mean', 'X_mean', 'X_max','Z_mean', 'X_jerk_max', 'X_jerk_range', 'X_range'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4f6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6bf67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.drop(['label'], axis=1)\n",
    "y=data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9484ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=StandardScaler()\n",
    "x=scale.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e292cd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2614, 33), (1121, 33))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_t, y_train, y_t= train_test_split(x,y,test_size=0.3, random_state=42)\n",
    "x_train.shape, x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659799ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((952, 33), (169, 33))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val, x_test, y_val, y_test= train_test_split(x_t, y_t, test_size=0.15, random_state=42)\n",
    "x_val.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f47d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 03:14:33.033480: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-18 03:14:33.079929: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-18 03:14:33.296543: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-18 03:14:33.296572: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-18 03:14:33.297873: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-18 03:14:33.416456: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-18 03:14:33.417771: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-18 03:14:34.287322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d89b654",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, num_heads, ff_dim, input_dim, dropout_rate, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(input_dim)  # Match the input dimension for residual connection\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Build Transformer Model for Tabular Data\n",
    "def build_transformer_model(input_dim, output_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Reshape to 3D for multi-head attention\n",
    "    x = layers.Reshape((1, input_dim))(inputs)\n",
    "    \n",
    "    # Stack Transformer Blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = TransformerEncoder(num_heads, ff_dim, input_dim, dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    # Output layer for classification\n",
    "    outputs = layers.Dense(output_dim, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a6f4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = x_train.shape[1]  # Number of features\n",
    "output_dim = len(y_train.unique())  # Number of classes for classification\n",
    "num_heads = 8\n",
    "ff_dim = 64\n",
    "num_transformer_blocks = 4\n",
    "dropout_rate = 0.1\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae5680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transformer_model(input_dim, output_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d06f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "82/82 [==============================] - 4s 10ms/step - loss: 1.5479 - accuracy: 0.4262 - val_loss: 1.4314 - val_accuracy: 0.4496\n",
      "Epoch 2/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 1.3000 - accuracy: 0.5249 - val_loss: 1.2349 - val_accuracy: 0.5231\n",
      "Epoch 3/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 1.2095 - accuracy: 0.5394 - val_loss: 1.2119 - val_accuracy: 0.5326\n",
      "Epoch 4/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.1470 - accuracy: 0.5700 - val_loss: 1.1327 - val_accuracy: 0.5809\n",
      "Epoch 5/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.1187 - accuracy: 0.5865 - val_loss: 1.0721 - val_accuracy: 0.5987\n",
      "Epoch 6/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.0574 - accuracy: 0.6171 - val_loss: 1.0377 - val_accuracy: 0.6103\n",
      "Epoch 7/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 1.0380 - accuracy: 0.6086 - val_loss: 1.0191 - val_accuracy: 0.6103\n",
      "Epoch 8/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.9736 - accuracy: 0.6450 - val_loss: 1.0339 - val_accuracy: 0.5977\n",
      "Epoch 9/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.9454 - accuracy: 0.6477 - val_loss: 0.9721 - val_accuracy: 0.6271\n",
      "Epoch 10/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.9228 - accuracy: 0.6630 - val_loss: 0.9428 - val_accuracy: 0.6376\n",
      "Epoch 11/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.9206 - accuracy: 0.6526 - val_loss: 0.9361 - val_accuracy: 0.6418\n",
      "Epoch 12/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.9051 - accuracy: 0.6756 - val_loss: 0.8912 - val_accuracy: 0.6586\n",
      "Epoch 13/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.8457 - accuracy: 0.6905 - val_loss: 0.9223 - val_accuracy: 0.6544\n",
      "Epoch 14/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.8584 - accuracy: 0.6913 - val_loss: 0.8617 - val_accuracy: 0.6628\n",
      "Epoch 15/800\n",
      "82/82 [==============================] - 0s 5ms/step - loss: 0.8766 - accuracy: 0.6748 - val_loss: 0.9160 - val_accuracy: 0.6523\n",
      "Epoch 16/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.8062 - accuracy: 0.7066 - val_loss: 0.9251 - val_accuracy: 0.6649\n",
      "Epoch 17/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.8022 - accuracy: 0.7066 - val_loss: 0.8583 - val_accuracy: 0.6859\n",
      "Epoch 18/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.7864 - accuracy: 0.7154 - val_loss: 0.8291 - val_accuracy: 0.6849\n",
      "Epoch 19/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.7659 - accuracy: 0.7165 - val_loss: 0.8277 - val_accuracy: 0.7143\n",
      "Epoch 20/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.7399 - accuracy: 0.7372 - val_loss: 0.8494 - val_accuracy: 0.6880\n",
      "Epoch 21/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.7052 - accuracy: 0.7483 - val_loss: 0.8037 - val_accuracy: 0.7227\n",
      "Epoch 22/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.7261 - accuracy: 0.7349 - val_loss: 0.8121 - val_accuracy: 0.7017\n",
      "Epoch 23/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.7132 - accuracy: 0.7353 - val_loss: 0.8293 - val_accuracy: 0.6975\n",
      "Epoch 24/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6850 - accuracy: 0.7529 - val_loss: 0.8213 - val_accuracy: 0.7122\n",
      "Epoch 25/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.7081 - accuracy: 0.7406 - val_loss: 0.9118 - val_accuracy: 0.6775\n",
      "Epoch 26/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6986 - accuracy: 0.7464 - val_loss: 0.8162 - val_accuracy: 0.7216\n",
      "Epoch 27/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6537 - accuracy: 0.7628 - val_loss: 0.7721 - val_accuracy: 0.7332\n",
      "Epoch 28/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6412 - accuracy: 0.7697 - val_loss: 0.7628 - val_accuracy: 0.7290\n",
      "Epoch 29/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6340 - accuracy: 0.7670 - val_loss: 0.7990 - val_accuracy: 0.7143\n",
      "Epoch 30/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.6296 - accuracy: 0.7728 - val_loss: 0.7458 - val_accuracy: 0.7342\n",
      "Epoch 31/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.5819 - accuracy: 0.7835 - val_loss: 0.8001 - val_accuracy: 0.7279\n",
      "Epoch 32/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.6122 - accuracy: 0.7850 - val_loss: 0.7962 - val_accuracy: 0.7216\n",
      "Epoch 33/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.6017 - accuracy: 0.7808 - val_loss: 0.7162 - val_accuracy: 0.7405\n",
      "Epoch 34/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.5964 - accuracy: 0.7819 - val_loss: 0.7766 - val_accuracy: 0.7122\n",
      "Epoch 35/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.5640 - accuracy: 0.8053 - val_loss: 0.7400 - val_accuracy: 0.7363\n",
      "Epoch 36/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.5831 - accuracy: 0.7877 - val_loss: 0.7604 - val_accuracy: 0.7542\n",
      "Epoch 37/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.5291 - accuracy: 0.8114 - val_loss: 0.7926 - val_accuracy: 0.7342\n",
      "Epoch 38/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.5785 - accuracy: 0.7896 - val_loss: 0.7968 - val_accuracy: 0.7248\n",
      "Epoch 39/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.5433 - accuracy: 0.8018 - val_loss: 0.7785 - val_accuracy: 0.7405\n",
      "Epoch 40/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.5437 - accuracy: 0.8060 - val_loss: 0.7830 - val_accuracy: 0.7489\n",
      "Epoch 41/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.5064 - accuracy: 0.8137 - val_loss: 0.7858 - val_accuracy: 0.7311\n",
      "Epoch 42/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.5190 - accuracy: 0.8114 - val_loss: 0.7132 - val_accuracy: 0.7616\n",
      "Epoch 43/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4786 - accuracy: 0.8267 - val_loss: 0.7886 - val_accuracy: 0.7574\n",
      "Epoch 44/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.5001 - accuracy: 0.8110 - val_loss: 0.7612 - val_accuracy: 0.7353\n",
      "Epoch 45/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.5038 - accuracy: 0.8244 - val_loss: 0.7624 - val_accuracy: 0.7553\n",
      "Epoch 46/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4755 - accuracy: 0.8256 - val_loss: 0.7605 - val_accuracy: 0.7521\n",
      "Epoch 47/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4625 - accuracy: 0.8321 - val_loss: 0.7404 - val_accuracy: 0.7521\n",
      "Epoch 48/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.5038 - accuracy: 0.8145 - val_loss: 0.7255 - val_accuracy: 0.7668\n",
      "Epoch 49/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4764 - accuracy: 0.8271 - val_loss: 0.7913 - val_accuracy: 0.7353\n",
      "Epoch 50/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4811 - accuracy: 0.8301 - val_loss: 0.8142 - val_accuracy: 0.7374\n",
      "Epoch 51/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4630 - accuracy: 0.8236 - val_loss: 0.7512 - val_accuracy: 0.7500\n",
      "Epoch 52/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4406 - accuracy: 0.8412 - val_loss: 0.7527 - val_accuracy: 0.7605\n",
      "Epoch 53/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4186 - accuracy: 0.8481 - val_loss: 0.7639 - val_accuracy: 0.7679\n",
      "Epoch 54/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4400 - accuracy: 0.8428 - val_loss: 0.7226 - val_accuracy: 0.7794\n",
      "Epoch 55/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4264 - accuracy: 0.8435 - val_loss: 0.7922 - val_accuracy: 0.7405\n",
      "Epoch 56/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4514 - accuracy: 0.8412 - val_loss: 0.7539 - val_accuracy: 0.7468\n",
      "Epoch 57/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4433 - accuracy: 0.8344 - val_loss: 0.7936 - val_accuracy: 0.7563\n",
      "Epoch 58/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4170 - accuracy: 0.8447 - val_loss: 0.7292 - val_accuracy: 0.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3970 - accuracy: 0.8604 - val_loss: 0.7434 - val_accuracy: 0.7637\n",
      "Epoch 60/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3671 - accuracy: 0.8646 - val_loss: 0.7676 - val_accuracy: 0.7668\n",
      "Epoch 61/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3828 - accuracy: 0.8619 - val_loss: 0.8178 - val_accuracy: 0.7689\n",
      "Epoch 62/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.4005 - accuracy: 0.8527 - val_loss: 0.7982 - val_accuracy: 0.7584\n",
      "Epoch 63/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3917 - accuracy: 0.8596 - val_loss: 0.8755 - val_accuracy: 0.7511\n",
      "Epoch 64/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3732 - accuracy: 0.8615 - val_loss: 0.8317 - val_accuracy: 0.7637\n",
      "Epoch 65/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3953 - accuracy: 0.8550 - val_loss: 0.7769 - val_accuracy: 0.7553\n",
      "Epoch 66/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3936 - accuracy: 0.8523 - val_loss: 0.7541 - val_accuracy: 0.7857\n",
      "Epoch 67/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3748 - accuracy: 0.8680 - val_loss: 0.7778 - val_accuracy: 0.7584\n",
      "Epoch 68/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3620 - accuracy: 0.8657 - val_loss: 0.7827 - val_accuracy: 0.7700\n",
      "Epoch 69/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3495 - accuracy: 0.8699 - val_loss: 0.7689 - val_accuracy: 0.7647\n",
      "Epoch 70/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3337 - accuracy: 0.8745 - val_loss: 0.8119 - val_accuracy: 0.7689\n",
      "Epoch 71/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3789 - accuracy: 0.8577 - val_loss: 0.8281 - val_accuracy: 0.7532\n",
      "Epoch 72/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3473 - accuracy: 0.8669 - val_loss: 0.7272 - val_accuracy: 0.7784\n",
      "Epoch 73/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3323 - accuracy: 0.8757 - val_loss: 0.8692 - val_accuracy: 0.7416\n",
      "Epoch 74/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3246 - accuracy: 0.8761 - val_loss: 0.7488 - val_accuracy: 0.7647\n",
      "Epoch 75/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3337 - accuracy: 0.8757 - val_loss: 0.8200 - val_accuracy: 0.7658\n",
      "Epoch 76/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.3625 - accuracy: 0.8673 - val_loss: 0.7858 - val_accuracy: 0.7700\n",
      "Epoch 77/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.3408 - accuracy: 0.8699 - val_loss: 0.7245 - val_accuracy: 0.7815\n",
      "Epoch 78/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3003 - accuracy: 0.8868 - val_loss: 0.8535 - val_accuracy: 0.7447\n",
      "Epoch 79/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3234 - accuracy: 0.8783 - val_loss: 0.7566 - val_accuracy: 0.7805\n",
      "Epoch 80/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2998 - accuracy: 0.8894 - val_loss: 0.7786 - val_accuracy: 0.7763\n",
      "Epoch 81/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2978 - accuracy: 0.8929 - val_loss: 0.7758 - val_accuracy: 0.7742\n",
      "Epoch 82/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3380 - accuracy: 0.8757 - val_loss: 0.7797 - val_accuracy: 0.7689\n",
      "Epoch 83/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3279 - accuracy: 0.8791 - val_loss: 0.8299 - val_accuracy: 0.7584\n",
      "Epoch 84/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.3040 - accuracy: 0.8871 - val_loss: 0.7351 - val_accuracy: 0.7847\n",
      "Epoch 85/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.3144 - accuracy: 0.8826 - val_loss: 0.7651 - val_accuracy: 0.7826\n",
      "Epoch 86/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2761 - accuracy: 0.8990 - val_loss: 0.7665 - val_accuracy: 0.7857\n",
      "Epoch 87/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.2632 - accuracy: 0.9005 - val_loss: 0.8363 - val_accuracy: 0.7826\n",
      "Epoch 88/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2857 - accuracy: 0.9047 - val_loss: 0.7391 - val_accuracy: 0.7973\n",
      "Epoch 89/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.2854 - accuracy: 0.8894 - val_loss: 0.7730 - val_accuracy: 0.7920\n",
      "Epoch 90/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.3026 - accuracy: 0.8875 - val_loss: 0.9059 - val_accuracy: 0.7616\n",
      "Epoch 91/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.3094 - accuracy: 0.8841 - val_loss: 0.8111 - val_accuracy: 0.7742\n",
      "Epoch 92/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.3090 - accuracy: 0.8910 - val_loss: 0.8043 - val_accuracy: 0.7763\n",
      "Epoch 93/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.2986 - accuracy: 0.8902 - val_loss: 0.8581 - val_accuracy: 0.7752\n",
      "Epoch 94/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2715 - accuracy: 0.8956 - val_loss: 0.8448 - val_accuracy: 0.7637\n",
      "Epoch 95/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2627 - accuracy: 0.9017 - val_loss: 0.8217 - val_accuracy: 0.7742\n",
      "Epoch 96/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2743 - accuracy: 0.9013 - val_loss: 0.8114 - val_accuracy: 0.7805\n",
      "Epoch 97/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2874 - accuracy: 0.8906 - val_loss: 0.7989 - val_accuracy: 0.7794\n",
      "Epoch 98/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2688 - accuracy: 0.8994 - val_loss: 0.8286 - val_accuracy: 0.7668\n",
      "Epoch 99/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.3065 - accuracy: 0.8833 - val_loss: 0.8342 - val_accuracy: 0.7679\n",
      "Epoch 100/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2608 - accuracy: 0.9002 - val_loss: 0.8203 - val_accuracy: 0.7742\n",
      "Epoch 101/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2578 - accuracy: 0.8986 - val_loss: 0.9188 - val_accuracy: 0.7574\n",
      "Epoch 102/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.2484 - accuracy: 0.9093 - val_loss: 0.8164 - val_accuracy: 0.7710\n",
      "Epoch 103/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2778 - accuracy: 0.8986 - val_loss: 0.8448 - val_accuracy: 0.7857\n",
      "Epoch 104/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2572 - accuracy: 0.9067 - val_loss: 0.8440 - val_accuracy: 0.7647\n",
      "Epoch 105/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2362 - accuracy: 0.9139 - val_loss: 0.8275 - val_accuracy: 0.7700\n",
      "Epoch 106/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2448 - accuracy: 0.9044 - val_loss: 0.8056 - val_accuracy: 0.7962\n",
      "Epoch 107/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2327 - accuracy: 0.9093 - val_loss: 0.8528 - val_accuracy: 0.7847\n",
      "Epoch 108/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2292 - accuracy: 0.9170 - val_loss: 0.7883 - val_accuracy: 0.7889\n",
      "Epoch 109/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2656 - accuracy: 0.9028 - val_loss: 0.7672 - val_accuracy: 0.7920\n",
      "Epoch 110/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2305 - accuracy: 0.9139 - val_loss: 0.8645 - val_accuracy: 0.7752\n",
      "Epoch 111/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2284 - accuracy: 0.9158 - val_loss: 0.8625 - val_accuracy: 0.7952\n",
      "Epoch 112/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2420 - accuracy: 0.9047 - val_loss: 0.8492 - val_accuracy: 0.7878\n",
      "Epoch 113/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2502 - accuracy: 0.9024 - val_loss: 0.8141 - val_accuracy: 0.7836\n",
      "Epoch 114/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2534 - accuracy: 0.9059 - val_loss: 0.8119 - val_accuracy: 0.7847\n",
      "Epoch 115/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2156 - accuracy: 0.9227 - val_loss: 0.8644 - val_accuracy: 0.7878\n",
      "Epoch 116/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2304 - accuracy: 0.9193 - val_loss: 0.8114 - val_accuracy: 0.7805\n",
      "Epoch 117/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2300 - accuracy: 0.9132 - val_loss: 0.8533 - val_accuracy: 0.7836\n",
      "Epoch 118/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2442 - accuracy: 0.9158 - val_loss: 0.8810 - val_accuracy: 0.7889\n",
      "Epoch 119/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2277 - accuracy: 0.9189 - val_loss: 0.8756 - val_accuracy: 0.7826\n",
      "Epoch 120/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2194 - accuracy: 0.9135 - val_loss: 0.8749 - val_accuracy: 0.7815\n",
      "Epoch 121/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2395 - accuracy: 0.9086 - val_loss: 0.8599 - val_accuracy: 0.7710\n",
      "Epoch 122/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2379 - accuracy: 0.9105 - val_loss: 0.9210 - val_accuracy: 0.7710\n",
      "Epoch 123/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2249 - accuracy: 0.9162 - val_loss: 0.8042 - val_accuracy: 0.7815\n",
      "Epoch 124/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2391 - accuracy: 0.9178 - val_loss: 0.8938 - val_accuracy: 0.7721\n",
      "Epoch 125/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2014 - accuracy: 0.9273 - val_loss: 0.8868 - val_accuracy: 0.7773\n",
      "Epoch 126/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1902 - accuracy: 0.9281 - val_loss: 0.8771 - val_accuracy: 0.7805\n",
      "Epoch 127/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2127 - accuracy: 0.9197 - val_loss: 0.8860 - val_accuracy: 0.7878\n",
      "Epoch 128/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2008 - accuracy: 0.9285 - val_loss: 0.8620 - val_accuracy: 0.7962\n",
      "Epoch 129/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2133 - accuracy: 0.9208 - val_loss: 0.9822 - val_accuracy: 0.7700\n",
      "Epoch 130/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1714 - accuracy: 0.9357 - val_loss: 0.9006 - val_accuracy: 0.7794\n",
      "Epoch 131/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2277 - accuracy: 0.9120 - val_loss: 0.8622 - val_accuracy: 0.7889\n",
      "Epoch 132/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2186 - accuracy: 0.9204 - val_loss: 0.9176 - val_accuracy: 0.7773\n",
      "Epoch 133/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2035 - accuracy: 0.9243 - val_loss: 0.8744 - val_accuracy: 0.7983\n",
      "Epoch 134/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1813 - accuracy: 0.9361 - val_loss: 0.9590 - val_accuracy: 0.7637\n",
      "Epoch 135/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1970 - accuracy: 0.9334 - val_loss: 0.9555 - val_accuracy: 0.7731\n",
      "Epoch 136/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2156 - accuracy: 0.9178 - val_loss: 0.9073 - val_accuracy: 0.7679\n",
      "Epoch 137/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2543 - accuracy: 0.9082 - val_loss: 0.8959 - val_accuracy: 0.7899\n",
      "Epoch 138/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2060 - accuracy: 0.9239 - val_loss: 0.9354 - val_accuracy: 0.7689\n",
      "Epoch 139/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1940 - accuracy: 0.9277 - val_loss: 0.8937 - val_accuracy: 0.7752\n",
      "Epoch 140/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1685 - accuracy: 0.9380 - val_loss: 0.9126 - val_accuracy: 0.7794\n",
      "Epoch 141/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1809 - accuracy: 0.9327 - val_loss: 0.9432 - val_accuracy: 0.7742\n",
      "Epoch 142/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1961 - accuracy: 0.9296 - val_loss: 0.9274 - val_accuracy: 0.7805\n",
      "Epoch 143/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2029 - accuracy: 0.9262 - val_loss: 0.9880 - val_accuracy: 0.7647\n",
      "Epoch 144/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2022 - accuracy: 0.9265 - val_loss: 0.9593 - val_accuracy: 0.7763\n",
      "Epoch 145/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1923 - accuracy: 0.9304 - val_loss: 0.9716 - val_accuracy: 0.7742\n",
      "Epoch 146/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1840 - accuracy: 0.9308 - val_loss: 0.9324 - val_accuracy: 0.7742\n",
      "Epoch 147/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1883 - accuracy: 0.9296 - val_loss: 0.9908 - val_accuracy: 0.7521\n",
      "Epoch 148/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1711 - accuracy: 0.9388 - val_loss: 0.9112 - val_accuracy: 0.7847\n",
      "Epoch 149/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1731 - accuracy: 0.9346 - val_loss: 0.9242 - val_accuracy: 0.7784\n",
      "Epoch 150/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1911 - accuracy: 0.9277 - val_loss: 0.9356 - val_accuracy: 0.7794\n",
      "Epoch 151/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1799 - accuracy: 0.9365 - val_loss: 1.0045 - val_accuracy: 0.7689\n",
      "Epoch 152/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1689 - accuracy: 0.9369 - val_loss: 0.9689 - val_accuracy: 0.7689\n",
      "Epoch 153/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.9430 - val_loss: 1.0274 - val_accuracy: 0.7679\n",
      "Epoch 154/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2000 - accuracy: 0.9250 - val_loss: 0.9534 - val_accuracy: 0.7826\n",
      "Epoch 155/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1919 - accuracy: 0.9258 - val_loss: 1.0116 - val_accuracy: 0.7668\n",
      "Epoch 156/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2002 - accuracy: 0.9281 - val_loss: 0.9347 - val_accuracy: 0.7731\n",
      "Epoch 157/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.2467 - accuracy: 0.9185 - val_loss: 0.9702 - val_accuracy: 0.7773\n",
      "Epoch 158/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1960 - accuracy: 0.9300 - val_loss: 0.9376 - val_accuracy: 0.7857\n",
      "Epoch 159/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1631 - accuracy: 0.9376 - val_loss: 0.9374 - val_accuracy: 0.7889\n",
      "Epoch 160/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1626 - accuracy: 0.9350 - val_loss: 0.9551 - val_accuracy: 0.7742\n",
      "Epoch 161/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1728 - accuracy: 0.9380 - val_loss: 0.9993 - val_accuracy: 0.7763\n",
      "Epoch 162/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1576 - accuracy: 0.9411 - val_loss: 0.9314 - val_accuracy: 0.7857\n",
      "Epoch 163/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1550 - accuracy: 0.9399 - val_loss: 0.9529 - val_accuracy: 0.7899\n",
      "Epoch 164/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1546 - accuracy: 0.9419 - val_loss: 1.0767 - val_accuracy: 0.7689\n",
      "Epoch 165/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1577 - accuracy: 0.9422 - val_loss: 0.9469 - val_accuracy: 0.7899\n",
      "Epoch 166/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1429 - accuracy: 0.9507 - val_loss: 1.0212 - val_accuracy: 0.7668\n",
      "Epoch 167/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1613 - accuracy: 0.9369 - val_loss: 1.0222 - val_accuracy: 0.7763\n",
      "Epoch 168/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1656 - accuracy: 0.9392 - val_loss: 1.0346 - val_accuracy: 0.7773\n",
      "Epoch 169/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1596 - accuracy: 0.9369 - val_loss: 1.0251 - val_accuracy: 0.7742\n",
      "Epoch 170/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9491 - val_loss: 0.9352 - val_accuracy: 0.7931\n",
      "Epoch 171/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9411 - val_loss: 0.9614 - val_accuracy: 0.7857\n",
      "Epoch 172/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1734 - accuracy: 0.9388 - val_loss: 0.9482 - val_accuracy: 0.7847\n",
      "Epoch 173/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1661 - accuracy: 0.9384 - val_loss: 1.0240 - val_accuracy: 0.7857\n",
      "Epoch 174/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1661 - accuracy: 0.9392 - val_loss: 1.0364 - val_accuracy: 0.7616\n",
      "Epoch 175/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1631 - accuracy: 0.9396 - val_loss: 1.0317 - val_accuracy: 0.7763\n",
      "Epoch 176/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1644 - accuracy: 0.9365 - val_loss: 1.0063 - val_accuracy: 0.7700\n",
      "Epoch 177/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1929 - accuracy: 0.9357 - val_loss: 1.0515 - val_accuracy: 0.7794\n",
      "Epoch 178/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1720 - accuracy: 0.9342 - val_loss: 0.9859 - val_accuracy: 0.7815\n",
      "Epoch 179/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1727 - accuracy: 0.9361 - val_loss: 0.9465 - val_accuracy: 0.7931\n",
      "Epoch 180/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1564 - accuracy: 0.9453 - val_loss: 0.9468 - val_accuracy: 0.7857\n",
      "Epoch 181/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1307 - accuracy: 0.9560 - val_loss: 0.9253 - val_accuracy: 0.7889\n",
      "Epoch 182/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1716 - accuracy: 0.9399 - val_loss: 0.9397 - val_accuracy: 0.7815\n",
      "Epoch 183/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1408 - accuracy: 0.9514 - val_loss: 0.9646 - val_accuracy: 0.7794\n",
      "Epoch 184/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9491 - val_loss: 1.0342 - val_accuracy: 0.7679\n",
      "Epoch 185/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1862 - accuracy: 0.9346 - val_loss: 0.9819 - val_accuracy: 0.7710\n",
      "Epoch 186/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1443 - accuracy: 0.9449 - val_loss: 0.9208 - val_accuracy: 0.7962\n",
      "Epoch 187/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1325 - accuracy: 0.9529 - val_loss: 0.9968 - val_accuracy: 0.7805\n",
      "Epoch 188/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1368 - accuracy: 0.9487 - val_loss: 1.0105 - val_accuracy: 0.7815\n",
      "Epoch 189/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1398 - accuracy: 0.9438 - val_loss: 0.9769 - val_accuracy: 0.7794\n",
      "Epoch 190/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1442 - accuracy: 0.9461 - val_loss: 1.0068 - val_accuracy: 0.7616\n",
      "Epoch 191/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.9411 - val_loss: 1.0293 - val_accuracy: 0.7721\n",
      "Epoch 192/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1602 - accuracy: 0.9376 - val_loss: 0.9902 - val_accuracy: 0.7815\n",
      "Epoch 193/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1745 - accuracy: 0.9361 - val_loss: 1.0334 - val_accuracy: 0.7700\n",
      "Epoch 194/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1454 - accuracy: 0.9484 - val_loss: 1.0615 - val_accuracy: 0.7553\n",
      "Epoch 195/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1375 - accuracy: 0.9503 - val_loss: 0.9734 - val_accuracy: 0.7668\n",
      "Epoch 196/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1488 - accuracy: 0.9495 - val_loss: 1.0006 - val_accuracy: 0.7752\n",
      "Epoch 197/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1433 - accuracy: 0.9434 - val_loss: 0.9795 - val_accuracy: 0.7931\n",
      "Epoch 198/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1432 - accuracy: 0.9449 - val_loss: 1.0356 - val_accuracy: 0.7805\n",
      "Epoch 199/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1649 - accuracy: 0.9415 - val_loss: 1.0284 - val_accuracy: 0.7658\n",
      "Epoch 200/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1628 - accuracy: 0.9438 - val_loss: 1.0073 - val_accuracy: 0.7836\n",
      "Epoch 201/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1505 - accuracy: 0.9407 - val_loss: 0.9454 - val_accuracy: 0.8004\n",
      "Epoch 202/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1494 - accuracy: 0.9499 - val_loss: 0.9958 - val_accuracy: 0.7731\n",
      "Epoch 203/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1587 - accuracy: 0.9422 - val_loss: 0.9588 - val_accuracy: 0.7847\n",
      "Epoch 204/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1257 - accuracy: 0.9510 - val_loss: 1.0122 - val_accuracy: 0.7742\n",
      "Epoch 205/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1382 - accuracy: 0.9507 - val_loss: 1.0103 - val_accuracy: 0.7836\n",
      "Epoch 206/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1389 - accuracy: 0.9480 - val_loss: 0.9909 - val_accuracy: 0.7994\n",
      "Epoch 207/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1336 - accuracy: 0.9503 - val_loss: 1.0508 - val_accuracy: 0.7836\n",
      "Epoch 208/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.9491 - val_loss: 1.0249 - val_accuracy: 0.7857\n",
      "Epoch 209/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1477 - accuracy: 0.9464 - val_loss: 1.0348 - val_accuracy: 0.7805\n",
      "Epoch 210/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1274 - accuracy: 0.9537 - val_loss: 1.0147 - val_accuracy: 0.7857\n",
      "Epoch 211/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1599 - accuracy: 0.9392 - val_loss: 0.9885 - val_accuracy: 0.7952\n",
      "Epoch 212/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1241 - accuracy: 0.9514 - val_loss: 0.9921 - val_accuracy: 0.7763\n",
      "Epoch 213/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1640 - accuracy: 0.9419 - val_loss: 1.0812 - val_accuracy: 0.7752\n",
      "Epoch 214/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1587 - accuracy: 0.9384 - val_loss: 1.0407 - val_accuracy: 0.7710\n",
      "Epoch 215/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1531 - accuracy: 0.9434 - val_loss: 0.9928 - val_accuracy: 0.7731\n",
      "Epoch 216/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.1351 - accuracy: 0.9491 - val_loss: 0.9875 - val_accuracy: 0.7784\n",
      "Epoch 217/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1553 - accuracy: 0.9419 - val_loss: 0.9968 - val_accuracy: 0.7742\n",
      "Epoch 218/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1169 - accuracy: 0.9617 - val_loss: 1.0326 - val_accuracy: 0.7637\n",
      "Epoch 219/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1397 - accuracy: 0.9507 - val_loss: 1.0611 - val_accuracy: 0.7731\n",
      "Epoch 220/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1203 - accuracy: 0.9545 - val_loss: 1.0113 - val_accuracy: 0.7857\n",
      "Epoch 221/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1259 - accuracy: 0.9507 - val_loss: 1.0841 - val_accuracy: 0.7689\n",
      "Epoch 222/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.1287 - accuracy: 0.9560 - val_loss: 1.0386 - val_accuracy: 0.7899\n",
      "Epoch 223/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1371 - accuracy: 0.9484 - val_loss: 1.0744 - val_accuracy: 0.7721\n",
      "Epoch 224/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1215 - accuracy: 0.9545 - val_loss: 1.0137 - val_accuracy: 0.7773\n",
      "Epoch 225/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.0869 - accuracy: 0.9690 - val_loss: 1.0188 - val_accuracy: 0.7983\n",
      "Epoch 226/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1194 - accuracy: 0.9564 - val_loss: 1.1092 - val_accuracy: 0.7626\n",
      "Epoch 227/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.1667 - accuracy: 0.9373 - val_loss: 1.0882 - val_accuracy: 0.7836\n",
      "Epoch 228/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1913 - accuracy: 0.9277 - val_loss: 1.0612 - val_accuracy: 0.7700\n",
      "Epoch 229/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1684 - accuracy: 0.9403 - val_loss: 1.0469 - val_accuracy: 0.7679\n",
      "Epoch 230/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1342 - accuracy: 0.9499 - val_loss: 1.0098 - val_accuracy: 0.7794\n",
      "Epoch 231/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1108 - accuracy: 0.9606 - val_loss: 1.0255 - val_accuracy: 0.7931\n",
      "Epoch 232/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1123 - accuracy: 0.9568 - val_loss: 0.9505 - val_accuracy: 0.7983\n",
      "Epoch 233/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1328 - accuracy: 0.9541 - val_loss: 0.9741 - val_accuracy: 0.7847\n",
      "Epoch 234/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1266 - accuracy: 0.9552 - val_loss: 0.9990 - val_accuracy: 0.7752\n",
      "Epoch 235/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1359 - accuracy: 0.9510 - val_loss: 1.0017 - val_accuracy: 0.7868\n",
      "Epoch 236/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1184 - accuracy: 0.9572 - val_loss: 0.9908 - val_accuracy: 0.7920\n",
      "Epoch 237/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1090 - accuracy: 0.9602 - val_loss: 1.0685 - val_accuracy: 0.7763\n",
      "Epoch 238/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1103 - accuracy: 0.9606 - val_loss: 1.1458 - val_accuracy: 0.7773\n",
      "Epoch 239/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1477 - accuracy: 0.9457 - val_loss: 1.0496 - val_accuracy: 0.7805\n",
      "Epoch 240/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1555 - accuracy: 0.9419 - val_loss: 1.0401 - val_accuracy: 0.7857\n",
      "Epoch 241/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1407 - accuracy: 0.9487 - val_loss: 1.0528 - val_accuracy: 0.7689\n",
      "Epoch 242/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1478 - accuracy: 0.9503 - val_loss: 1.1525 - val_accuracy: 0.7647\n",
      "Epoch 243/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1481 - accuracy: 0.9468 - val_loss: 0.9749 - val_accuracy: 0.7941\n",
      "Epoch 244/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1251 - accuracy: 0.9610 - val_loss: 1.0100 - val_accuracy: 0.7815\n",
      "Epoch 245/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1243 - accuracy: 0.9552 - val_loss: 1.0061 - val_accuracy: 0.7836\n",
      "Epoch 246/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.0981 - accuracy: 0.9679 - val_loss: 1.0112 - val_accuracy: 0.7973\n",
      "Epoch 247/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.0967 - accuracy: 0.9606 - val_loss: 0.9823 - val_accuracy: 0.7973\n",
      "Epoch 248/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1071 - accuracy: 0.9621 - val_loss: 1.0293 - val_accuracy: 0.7847\n",
      "Epoch 249/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1425 - accuracy: 0.9468 - val_loss: 1.0451 - val_accuracy: 0.7752\n",
      "Epoch 250/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1432 - accuracy: 0.9491 - val_loss: 0.9793 - val_accuracy: 0.8078\n",
      "Epoch 251/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1314 - accuracy: 0.9514 - val_loss: 1.0317 - val_accuracy: 0.7857\n",
      "Epoch 252/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1301 - accuracy: 0.9487 - val_loss: 0.9825 - val_accuracy: 0.7868\n",
      "Epoch 253/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1194 - accuracy: 0.9545 - val_loss: 1.0386 - val_accuracy: 0.7899\n",
      "Epoch 254/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9564 - val_loss: 0.9569 - val_accuracy: 0.7899\n",
      "Epoch 255/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1125 - accuracy: 0.9594 - val_loss: 0.9770 - val_accuracy: 0.7878\n",
      "Epoch 256/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1108 - accuracy: 0.9541 - val_loss: 1.0069 - val_accuracy: 0.7857\n",
      "Epoch 257/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1321 - accuracy: 0.9510 - val_loss: 0.9753 - val_accuracy: 0.7962\n",
      "Epoch 258/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1187 - accuracy: 0.9560 - val_loss: 1.0898 - val_accuracy: 0.7721\n",
      "Epoch 259/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1414 - accuracy: 0.9426 - val_loss: 0.9932 - val_accuracy: 0.8015\n",
      "Epoch 260/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9625 - val_loss: 1.0740 - val_accuracy: 0.7731\n",
      "Epoch 261/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1498 - accuracy: 0.9484 - val_loss: 1.0340 - val_accuracy: 0.7941\n",
      "Epoch 262/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1261 - accuracy: 0.9541 - val_loss: 1.0458 - val_accuracy: 0.7847\n",
      "Epoch 263/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1068 - accuracy: 0.9587 - val_loss: 1.1114 - val_accuracy: 0.7878\n",
      "Epoch 264/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1257 - accuracy: 0.9537 - val_loss: 1.0608 - val_accuracy: 0.7857\n",
      "Epoch 265/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1361 - accuracy: 0.9499 - val_loss: 1.1186 - val_accuracy: 0.7626\n",
      "Epoch 266/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1605 - accuracy: 0.9384 - val_loss: 1.1410 - val_accuracy: 0.7637\n",
      "Epoch 267/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1283 - accuracy: 0.9522 - val_loss: 1.0149 - val_accuracy: 0.7815\n",
      "Epoch 268/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1324 - accuracy: 0.9529 - val_loss: 0.9580 - val_accuracy: 0.8015\n",
      "Epoch 269/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.0934 - accuracy: 0.9640 - val_loss: 1.0743 - val_accuracy: 0.7847\n",
      "Epoch 270/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1100 - accuracy: 0.9621 - val_loss: 1.0397 - val_accuracy: 0.7815\n",
      "Epoch 271/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9484 - val_loss: 0.9730 - val_accuracy: 0.7920\n",
      "Epoch 272/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.0924 - accuracy: 0.9648 - val_loss: 1.0111 - val_accuracy: 0.7920\n",
      "Epoch 273/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 1.0960 - val_accuracy: 0.7847\n",
      "Epoch 274/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.0871 - accuracy: 0.9667 - val_loss: 1.0636 - val_accuracy: 0.7868\n",
      "Epoch 275/800\n",
      "82/82 [==============================] - 1s 7ms/step - loss: 0.1045 - accuracy: 0.9663 - val_loss: 1.1296 - val_accuracy: 0.7637\n",
      "Epoch 276/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1223 - accuracy: 0.9591 - val_loss: 1.0262 - val_accuracy: 0.7868\n",
      "Epoch 277/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1336 - accuracy: 0.9518 - val_loss: 1.0752 - val_accuracy: 0.7899\n",
      "Epoch 278/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.0881 - accuracy: 0.9667 - val_loss: 1.0456 - val_accuracy: 0.7847\n",
      "Epoch 279/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1232 - accuracy: 0.9560 - val_loss: 1.1378 - val_accuracy: 0.7710\n",
      "Epoch 280/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.1497 - accuracy: 0.9441 - val_loss: 1.0528 - val_accuracy: 0.7805\n",
      "Epoch 281/800\n",
      "82/82 [==============================] - 0s 6ms/step - loss: 0.0999 - accuracy: 0.9617 - val_loss: 1.0457 - val_accuracy: 0.7920\n",
      "Epoch 282/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1247 - accuracy: 0.9526 - val_loss: 1.1140 - val_accuracy: 0.7983\n",
      "Epoch 283/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1344 - accuracy: 0.9495 - val_loss: 1.0107 - val_accuracy: 0.7920\n",
      "Epoch 284/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1284 - accuracy: 0.9537 - val_loss: 1.0800 - val_accuracy: 0.7647\n",
      "Epoch 285/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.0946 - accuracy: 0.9660 - val_loss: 1.0402 - val_accuracy: 0.7868\n",
      "Epoch 286/800\n",
      "82/82 [==============================] - 1s 6ms/step - loss: 0.1266 - accuracy: 0.9522 - val_loss: 1.0772 - val_accuracy: 0.7689\n",
      "Epoch 287/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/82 [=================>............] - ETA: 0s - loss: 0.1365 - accuracy: 0.9461"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data= (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7295ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97808ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Epoch vs Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b427c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e787f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "if len(y_test.shape) == 1 or y_test.shape[1] == 1:\n",
    "    y_true = y_test  \n",
    "else:\n",
    "    y_true = np.argmax(y_test, axis=1)  \n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Purples')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "class_report = classification_report(y_true, y_pred_classes, target_names=[str(i) for i in range(len(np.unique(y_true)))])\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa16eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
