{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9528f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f8b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('ML_Data_CW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3723713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>X_min</th>\n",
       "      <th>X_max</th>\n",
       "      <th>X_range</th>\n",
       "      <th>X_mean</th>\n",
       "      <th>X_rms</th>\n",
       "      <th>X_std</th>\n",
       "      <th>Y_min</th>\n",
       "      <th>Y_max</th>\n",
       "      <th>Y_range</th>\n",
       "      <th>...</th>\n",
       "      <th>Z_jerk_mean</th>\n",
       "      <th>Z_jerk_rms</th>\n",
       "      <th>Z_jerk_std</th>\n",
       "      <th>total_jerk_min</th>\n",
       "      <th>total_jerk_max</th>\n",
       "      <th>total_jerk_range</th>\n",
       "      <th>total_jerk_mean</th>\n",
       "      <th>total_jerk_rms</th>\n",
       "      <th>total_jerk_std</th>\n",
       "      <th>timestamp_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.589872</td>\n",
       "      <td>0.347272</td>\n",
       "      <td>0.937143</td>\n",
       "      <td>-0.194542</td>\n",
       "      <td>0.268091</td>\n",
       "      <td>0.185794</td>\n",
       "      <td>-1.267788</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>1.272193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275866</td>\n",
       "      <td>16.042870</td>\n",
       "      <td>16.156316</td>\n",
       "      <td>-24.266511</td>\n",
       "      <td>29.783611</td>\n",
       "      <td>54.050122</td>\n",
       "      <td>-0.272102</td>\n",
       "      <td>11.621308</td>\n",
       "      <td>11.702008</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.678279</td>\n",
       "      <td>0.259369</td>\n",
       "      <td>0.937648</td>\n",
       "      <td>-0.200295</td>\n",
       "      <td>0.283716</td>\n",
       "      <td>0.202370</td>\n",
       "      <td>-2.018337</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>2.174288</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017593</td>\n",
       "      <td>19.087411</td>\n",
       "      <td>19.223258</td>\n",
       "      <td>-47.033978</td>\n",
       "      <td>40.079267</td>\n",
       "      <td>87.113246</td>\n",
       "      <td>0.439634</td>\n",
       "      <td>14.617773</td>\n",
       "      <td>14.715156</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.703621</td>\n",
       "      <td>0.355244</td>\n",
       "      <td>1.058865</td>\n",
       "      <td>-0.209411</td>\n",
       "      <td>0.300929</td>\n",
       "      <td>0.217460</td>\n",
       "      <td>-1.167808</td>\n",
       "      <td>-0.012946</td>\n",
       "      <td>1.154862</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017398</td>\n",
       "      <td>12.942794</td>\n",
       "      <td>12.983137</td>\n",
       "      <td>-23.432654</td>\n",
       "      <td>30.300758</td>\n",
       "      <td>53.733412</td>\n",
       "      <td>0.677985</td>\n",
       "      <td>10.235956</td>\n",
       "      <td>10.277114</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.711522</td>\n",
       "      <td>0.555218</td>\n",
       "      <td>1.266740</td>\n",
       "      <td>-0.207122</td>\n",
       "      <td>0.319552</td>\n",
       "      <td>0.244856</td>\n",
       "      <td>-1.471754</td>\n",
       "      <td>-0.024032</td>\n",
       "      <td>1.447722</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.453922</td>\n",
       "      <td>12.462453</td>\n",
       "      <td>12.454471</td>\n",
       "      <td>-29.483934</td>\n",
       "      <td>22.118163</td>\n",
       "      <td>51.602097</td>\n",
       "      <td>-1.472199</td>\n",
       "      <td>10.735650</td>\n",
       "      <td>10.700486</td>\n",
       "      <td>0.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.676285</td>\n",
       "      <td>0.676534</td>\n",
       "      <td>1.352819</td>\n",
       "      <td>-0.070346</td>\n",
       "      <td>0.298983</td>\n",
       "      <td>0.292520</td>\n",
       "      <td>-1.130257</td>\n",
       "      <td>0.102537</td>\n",
       "      <td>1.232795</td>\n",
       "      <td>...</td>\n",
       "      <td>3.066248</td>\n",
       "      <td>16.538112</td>\n",
       "      <td>16.359362</td>\n",
       "      <td>-28.066213</td>\n",
       "      <td>35.876247</td>\n",
       "      <td>63.942460</td>\n",
       "      <td>2.549368</td>\n",
       "      <td>12.604363</td>\n",
       "      <td>12.425872</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.953433</td>\n",
       "      <td>0.250183</td>\n",
       "      <td>1.203616</td>\n",
       "      <td>-0.267446</td>\n",
       "      <td>0.335269</td>\n",
       "      <td>0.203024</td>\n",
       "      <td>-0.800211</td>\n",
       "      <td>0.143310</td>\n",
       "      <td>0.943521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247229</td>\n",
       "      <td>9.659509</td>\n",
       "      <td>9.696496</td>\n",
       "      <td>-22.074841</td>\n",
       "      <td>23.390863</td>\n",
       "      <td>45.465704</td>\n",
       "      <td>0.201233</td>\n",
       "      <td>6.968500</td>\n",
       "      <td>6.994557</td>\n",
       "      <td>1.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.633834</td>\n",
       "      <td>1.097093</td>\n",
       "      <td>1.730927</td>\n",
       "      <td>-0.075134</td>\n",
       "      <td>0.373065</td>\n",
       "      <td>0.367049</td>\n",
       "      <td>-1.246303</td>\n",
       "      <td>0.299869</td>\n",
       "      <td>1.546173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056797</td>\n",
       "      <td>14.140192</td>\n",
       "      <td>14.203063</td>\n",
       "      <td>-52.673864</td>\n",
       "      <td>46.050547</td>\n",
       "      <td>98.724410</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>14.968089</td>\n",
       "      <td>15.034762</td>\n",
       "      <td>1.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.385272</td>\n",
       "      <td>0.583667</td>\n",
       "      <td>1.968939</td>\n",
       "      <td>-0.247752</td>\n",
       "      <td>0.407022</td>\n",
       "      <td>0.324163</td>\n",
       "      <td>-1.960975</td>\n",
       "      <td>0.035151</td>\n",
       "      <td>1.996125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071263</td>\n",
       "      <td>15.528399</td>\n",
       "      <td>15.587391</td>\n",
       "      <td>-45.077351</td>\n",
       "      <td>54.306146</td>\n",
       "      <td>99.383498</td>\n",
       "      <td>0.082529</td>\n",
       "      <td>12.754323</td>\n",
       "      <td>12.802643</td>\n",
       "      <td>1.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.676367</td>\n",
       "      <td>0.664444</td>\n",
       "      <td>1.340811</td>\n",
       "      <td>-0.164913</td>\n",
       "      <td>0.298208</td>\n",
       "      <td>0.249576</td>\n",
       "      <td>-0.899431</td>\n",
       "      <td>0.297562</td>\n",
       "      <td>1.196992</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096257</td>\n",
       "      <td>9.774722</td>\n",
       "      <td>9.818178</td>\n",
       "      <td>-32.409254</td>\n",
       "      <td>25.582425</td>\n",
       "      <td>57.991680</td>\n",
       "      <td>-0.220048</td>\n",
       "      <td>8.565381</td>\n",
       "      <td>8.601038</td>\n",
       "      <td>1.221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.763620</td>\n",
       "      <td>0.523834</td>\n",
       "      <td>1.287454</td>\n",
       "      <td>-0.207417</td>\n",
       "      <td>0.350966</td>\n",
       "      <td>0.284425</td>\n",
       "      <td>-1.229060</td>\n",
       "      <td>0.328558</td>\n",
       "      <td>1.557618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035625</td>\n",
       "      <td>12.889697</td>\n",
       "      <td>12.949184</td>\n",
       "      <td>-47.800478</td>\n",
       "      <td>43.509234</td>\n",
       "      <td>91.309711</td>\n",
       "      <td>0.031827</td>\n",
       "      <td>13.728517</td>\n",
       "      <td>13.791891</td>\n",
       "      <td>1.188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2553 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     X_min     X_max   X_range    X_mean     X_rms     X_std  \\\n",
       "0         1 -0.589872  0.347272  0.937143 -0.194542  0.268091  0.185794   \n",
       "1         1 -0.678279  0.259369  0.937648 -0.200295  0.283716  0.202370   \n",
       "2         1 -0.703621  0.355244  1.058865 -0.209411  0.300929  0.217460   \n",
       "3         1 -0.711522  0.555218  1.266740 -0.207122  0.319552  0.244856   \n",
       "4         1 -0.676285  0.676534  1.352819 -0.070346  0.298983  0.292520   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "2548      4 -0.953433  0.250183  1.203616 -0.267446  0.335269  0.203024   \n",
       "2549      4 -0.633834  1.097093  1.730927 -0.075134  0.373065  0.367049   \n",
       "2550      4 -1.385272  0.583667  1.968939 -0.247752  0.407022  0.324163   \n",
       "2551      4 -0.676367  0.664444  1.340811 -0.164913  0.298208  0.249576   \n",
       "2552      4 -0.763620  0.523834  1.287454 -0.207417  0.350966  0.284425   \n",
       "\n",
       "         Y_min     Y_max   Y_range  ...  Z_jerk_mean  Z_jerk_rms  Z_jerk_std  \\\n",
       "0    -1.267788  0.004404  1.272193  ...    -0.275866   16.042870   16.156316   \n",
       "1    -2.018337  0.155951  2.174288  ...    -0.017593   19.087411   19.223258   \n",
       "2    -1.167808 -0.012946  1.154862  ...     1.017398   12.942794   12.983137   \n",
       "3    -1.471754 -0.024032  1.447722  ...    -1.453922   12.462453   12.454471   \n",
       "4    -1.130257  0.102537  1.232795  ...     3.066248   16.538112   16.359362   \n",
       "...        ...       ...       ...  ...          ...         ...         ...   \n",
       "2548 -0.800211  0.143310  0.943521  ...     0.247229    9.659509    9.696496   \n",
       "2549 -1.246303  0.299869  1.546173  ...    -0.056797   14.140192   14.203063   \n",
       "2550 -1.960975  0.035151  1.996125  ...     0.071263   15.528399   15.587391   \n",
       "2551 -0.899431  0.297562  1.196992  ...    -0.096257    9.774722    9.818178   \n",
       "2552 -1.229060  0.328558  1.557618  ...     0.035625   12.889697   12.949184   \n",
       "\n",
       "      total_jerk_min  total_jerk_max  total_jerk_range  total_jerk_mean  \\\n",
       "0         -24.266511       29.783611         54.050122        -0.272102   \n",
       "1         -47.033978       40.079267         87.113246         0.439634   \n",
       "2         -23.432654       30.300758         53.733412         0.677985   \n",
       "3         -29.483934       22.118163         51.602097        -1.472199   \n",
       "4         -28.066213       35.876247         63.942460         2.549368   \n",
       "...              ...             ...               ...              ...   \n",
       "2548      -22.074841       23.390863         45.465704         0.201233   \n",
       "2549      -52.673864       46.050547         98.724410         0.001120   \n",
       "2550      -45.077351       54.306146         99.383498         0.082529   \n",
       "2551      -32.409254       25.582425         57.991680        -0.220048   \n",
       "2552      -47.800478       43.509234         91.309711         0.031827   \n",
       "\n",
       "      total_jerk_rms  total_jerk_std  timestamp_range  \n",
       "0          11.621308       11.702008            0.759  \n",
       "1          14.617773       14.715156            0.770  \n",
       "2          10.235956       10.277114            0.880  \n",
       "3          10.735650       10.700486            0.880  \n",
       "4          12.604363       12.425872            0.825  \n",
       "...              ...             ...              ...  \n",
       "2548        6.968500        6.994557            1.320  \n",
       "2549       14.968089       15.034762            1.232  \n",
       "2550       12.754323       12.802643            1.441  \n",
       "2551        8.565381        8.601038            1.221  \n",
       "2552       13.728517       13.791891            1.188  \n",
       "\n",
       "[2553 rows x 44 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4f6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6bf67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.drop(['label'], axis=1)\n",
    "y=data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9484ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=StandardScaler()\n",
    "x=scale.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e292cd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1787, 43), (766, 43))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_t, y_train, y_t= train_test_split(x,y,test_size=0.3, random_state=42)\n",
    "x_train.shape, x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659799ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((651, 43), (115, 43))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val, x_test, y_val, y_test= train_test_split(x_t, y_t, test_size=0.15, random_state=42)\n",
    "x_val.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f47d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 20:05:29.965666: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-27 20:05:30.019482: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 20:05:30.294420: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-27 20:05:30.294462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-27 20:05:30.295848: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-27 20:05:30.440249: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 20:05:30.441940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 20:05:31.275990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7199b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu'), \n",
    "        Dropout(0.6),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(24, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='softmax')  # 8 output neurons for 8 classes\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "856c4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, input_dim):\n",
    "    # Define the combinations of epochs and batch sizes\n",
    "    epochs_list = [600, 800, 1000, 1200, 1600, 2000]\n",
    "    batch_sizes = [8, 16, 32, 64]\n",
    "    \n",
    "    results = []  # To store the results\n",
    "    \n",
    "    # Create a directory to save the plots\n",
    "    plot_dir = 'CW_Test'\n",
    "    if not os.path.exists(plot_dir):\n",
    "        os.makedirs(plot_dir)\n",
    "\n",
    "    for epochs in epochs_list:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Training with epochs={epochs}, batch_size={batch_size}\")\n",
    "\n",
    "            # Create a new instance of the model\n",
    "            model = create_model(input_dim)\n",
    "            model.compile(optimizer='Adam',\n",
    "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            # Train the model\n",
    "            history = model.fit(X_train, y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=batch_size,\n",
    "                                validation_split=0.2,  # 20% of training data for validation\n",
    "                                verbose=2)\n",
    "\n",
    "            # Evaluate on training data\n",
    "            train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "            # Evaluate on test data\n",
    "            test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "            # Predict on the test data\n",
    "            y_pred = model.predict(X_test, verbose=0)\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "            # Calculate precision, recall, and F1 score\n",
    "            precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "            # Store the results\n",
    "            results.append([epochs, batch_size, train_accuracy, test_accuracy, precision, recall, f1])\n",
    "\n",
    "            # Plot the loss and accuracy curves\n",
    "            epochs_range = range(epochs)\n",
    "            train_loss_values = history.history['loss']\n",
    "            val_loss_values = history.history['val_loss']\n",
    "            train_accuracy_values = history.history['accuracy']\n",
    "            val_accuracy_values = history.history['val_accuracy']\n",
    "\n",
    "            plt.figure(figsize=(14, 5))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs_range, train_loss_values, label='Training Loss')\n",
    "            plt.plot(epochs_range, val_loss_values, label='Validation Loss')\n",
    "            plt.title(f'Epoch vs Loss: {epochs}, Batch Size: {batch_size}')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs_range, train_accuracy_values, label='Training Accuracy')\n",
    "            plt.plot(epochs_range, val_accuracy_values, label='Validation Accuracy')\n",
    "            plt.title(f'Epoch vs Accuracy: {epochs}, Batch Size: {batch_size}')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot to the plot directory\n",
    "            plot_filename = f\"{plot_dir}/epochs_{epochs}_batchsize_{batch_size}.png\"\n",
    "            plt.savefig(plot_filename)\n",
    "            plt.close()  # Close the plot to free memory\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab720170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with epochs=600, batch_size=8\n",
      "Epoch 1/600\n",
      "179/179 - 1s - loss: 1.8254 - accuracy: 0.2659 - val_loss: 1.5138 - val_accuracy: 0.3827 - 754ms/epoch - 4ms/step\n",
      "Epoch 2/600\n",
      "179/179 - 0s - loss: 1.4405 - accuracy: 0.3688 - val_loss: 1.2837 - val_accuracy: 0.3743 - 153ms/epoch - 857us/step\n",
      "Epoch 3/600\n",
      "179/179 - 0s - loss: 1.3179 - accuracy: 0.3982 - val_loss: 1.2135 - val_accuracy: 0.4050 - 152ms/epoch - 850us/step\n",
      "Epoch 4/600\n",
      "179/179 - 0s - loss: 1.2531 - accuracy: 0.4283 - val_loss: 1.1470 - val_accuracy: 0.5950 - 151ms/epoch - 841us/step\n",
      "Epoch 5/600\n",
      "179/179 - 0s - loss: 1.2262 - accuracy: 0.4416 - val_loss: 1.0986 - val_accuracy: 0.5531 - 150ms/epoch - 839us/step\n",
      "Epoch 6/600\n",
      "179/179 - 0s - loss: 1.1679 - accuracy: 0.4598 - val_loss: 1.0710 - val_accuracy: 0.5950 - 149ms/epoch - 835us/step\n",
      "Epoch 7/600\n",
      "179/179 - 0s - loss: 1.1555 - accuracy: 0.4850 - val_loss: 0.9964 - val_accuracy: 0.6704 - 152ms/epoch - 851us/step\n",
      "Epoch 8/600\n",
      "179/179 - 0s - loss: 1.1076 - accuracy: 0.5031 - val_loss: 0.9854 - val_accuracy: 0.6425 - 150ms/epoch - 836us/step\n",
      "Epoch 9/600\n",
      "179/179 - 0s - loss: 1.0954 - accuracy: 0.5486 - val_loss: 0.9832 - val_accuracy: 0.5782 - 150ms/epoch - 836us/step\n",
      "Epoch 10/600\n",
      "179/179 - 0s - loss: 1.0548 - accuracy: 0.5500 - val_loss: 0.9081 - val_accuracy: 0.6676 - 152ms/epoch - 850us/step\n",
      "Epoch 11/600\n",
      "179/179 - 0s - loss: 1.0538 - accuracy: 0.5458 - val_loss: 0.8967 - val_accuracy: 0.6480 - 152ms/epoch - 847us/step\n",
      "Epoch 12/600\n",
      "179/179 - 0s - loss: 1.0331 - accuracy: 0.5675 - val_loss: 0.8746 - val_accuracy: 0.6648 - 152ms/epoch - 852us/step\n",
      "Epoch 13/600\n",
      "179/179 - 0s - loss: 1.0419 - accuracy: 0.5780 - val_loss: 0.8731 - val_accuracy: 0.6816 - 153ms/epoch - 853us/step\n",
      "Epoch 14/600\n",
      "179/179 - 0s - loss: 0.9879 - accuracy: 0.5745 - val_loss: 0.8181 - val_accuracy: 0.6816 - 151ms/epoch - 846us/step\n",
      "Epoch 15/600\n",
      "179/179 - 0s - loss: 0.9710 - accuracy: 0.5941 - val_loss: 0.8141 - val_accuracy: 0.6955 - 150ms/epoch - 840us/step\n",
      "Epoch 16/600\n",
      "179/179 - 0s - loss: 0.9550 - accuracy: 0.5990 - val_loss: 0.8180 - val_accuracy: 0.6899 - 152ms/epoch - 850us/step\n",
      "Epoch 17/600\n",
      "179/179 - 0s - loss: 0.9442 - accuracy: 0.6249 - val_loss: 0.7868 - val_accuracy: 0.7346 - 151ms/epoch - 842us/step\n",
      "Epoch 18/600\n",
      "179/179 - 0s - loss: 0.9632 - accuracy: 0.6130 - val_loss: 0.7752 - val_accuracy: 0.7179 - 151ms/epoch - 841us/step\n",
      "Epoch 19/600\n",
      "179/179 - 0s - loss: 0.9321 - accuracy: 0.6109 - val_loss: 0.7478 - val_accuracy: 0.7263 - 151ms/epoch - 842us/step\n",
      "Epoch 20/600\n",
      "179/179 - 0s - loss: 0.8855 - accuracy: 0.6235 - val_loss: 0.7349 - val_accuracy: 0.7235 - 152ms/epoch - 846us/step\n",
      "Epoch 21/600\n",
      "179/179 - 0s - loss: 0.8958 - accuracy: 0.6298 - val_loss: 0.7273 - val_accuracy: 0.7570 - 152ms/epoch - 850us/step\n",
      "Epoch 22/600\n",
      "179/179 - 0s - loss: 0.8895 - accuracy: 0.6333 - val_loss: 0.7693 - val_accuracy: 0.6983 - 151ms/epoch - 843us/step\n",
      "Epoch 23/600\n",
      "179/179 - 0s - loss: 0.8988 - accuracy: 0.6403 - val_loss: 0.7210 - val_accuracy: 0.7430 - 153ms/epoch - 852us/step\n",
      "Epoch 24/600\n",
      "179/179 - 0s - loss: 0.8576 - accuracy: 0.6592 - val_loss: 0.7005 - val_accuracy: 0.7402 - 151ms/epoch - 846us/step\n",
      "Epoch 25/600\n",
      "179/179 - 0s - loss: 0.8843 - accuracy: 0.6354 - val_loss: 0.7040 - val_accuracy: 0.7235 - 156ms/epoch - 871us/step\n",
      "Epoch 26/600\n",
      "179/179 - 0s - loss: 0.8445 - accuracy: 0.6627 - val_loss: 0.6863 - val_accuracy: 0.7346 - 151ms/epoch - 843us/step\n",
      "Epoch 27/600\n",
      "179/179 - 0s - loss: 0.8678 - accuracy: 0.6452 - val_loss: 0.7065 - val_accuracy: 0.7346 - 151ms/epoch - 845us/step\n",
      "Epoch 28/600\n",
      "179/179 - 0s - loss: 0.8472 - accuracy: 0.6557 - val_loss: 0.6815 - val_accuracy: 0.7374 - 150ms/epoch - 839us/step\n",
      "Epoch 29/600\n",
      "179/179 - 0s - loss: 0.8154 - accuracy: 0.6669 - val_loss: 0.6718 - val_accuracy: 0.7430 - 152ms/epoch - 849us/step\n",
      "Epoch 30/600\n",
      "179/179 - 0s - loss: 0.8309 - accuracy: 0.6669 - val_loss: 0.6640 - val_accuracy: 0.7430 - 151ms/epoch - 846us/step\n",
      "Epoch 31/600\n",
      "179/179 - 0s - loss: 0.8172 - accuracy: 0.6795 - val_loss: 0.6796 - val_accuracy: 0.7514 - 152ms/epoch - 852us/step\n",
      "Epoch 32/600\n",
      "179/179 - 0s - loss: 0.8173 - accuracy: 0.6767 - val_loss: 0.6551 - val_accuracy: 0.7765 - 150ms/epoch - 837us/step\n",
      "Epoch 33/600\n",
      "179/179 - 0s - loss: 0.8018 - accuracy: 0.6718 - val_loss: 0.6535 - val_accuracy: 0.7458 - 152ms/epoch - 848us/step\n",
      "Epoch 34/600\n",
      "179/179 - 0s - loss: 0.7874 - accuracy: 0.6872 - val_loss: 0.6641 - val_accuracy: 0.7598 - 163ms/epoch - 911us/step\n",
      "Epoch 35/600\n",
      "179/179 - 0s - loss: 0.8055 - accuracy: 0.6816 - val_loss: 0.6370 - val_accuracy: 0.7765 - 158ms/epoch - 885us/step\n",
      "Epoch 36/600\n",
      "179/179 - 0s - loss: 0.7993 - accuracy: 0.6802 - val_loss: 0.6379 - val_accuracy: 0.7598 - 164ms/epoch - 915us/step\n",
      "Epoch 37/600\n",
      "179/179 - 0s - loss: 0.7859 - accuracy: 0.6830 - val_loss: 0.6343 - val_accuracy: 0.7570 - 160ms/epoch - 892us/step\n",
      "Epoch 38/600\n"
     ]
    }
   ],
   "source": [
    "output=train_and_evaluate_model(x_train, y_train, x_test, y_test, x_train.shape[1])\n",
    "\n",
    "for result in output:\n",
    "    print(f\"Epochs: {result[0]}, Batch Size: {result[1]}, \"\n",
    "          f\"Train Accuracy: {result[2]:.4f}, Test Accuracy: {result[3]:.4f}, \"\n",
    "          f\"Precision: {result[4]:.4f}, Recall: {result[5]:.4f}, F1 Score: {result[6]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa903f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(output)\n",
    "df.to_csv('CW_Test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3d1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
